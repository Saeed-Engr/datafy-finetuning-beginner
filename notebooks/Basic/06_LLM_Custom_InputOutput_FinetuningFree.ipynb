{"cells":[{"cell_type":"markdown","metadata":{"id":"s-MyFJStxS9H"},"source":["<a href=\"https://colab.research.google.com/github/datafyresearcher/datafy-finetuning-beginner/blob/main/notebooks/Basic/06_LLM_Custom_InputOutput_FinetuningFree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"4dIzkKjG1f_R"},"source":["# [Lamini](https://www.lamini.ai/): The LLM engine for rapidly customizing models ğŸ¦™\n","Walk through Lamini's finetuning pipeline, so you can train custom models on your data.\n","\n","- It's free at $0 per training run.\n","- It's fast at less than 15 minutes.\n","- It's similar to a nearly unlimited prompt size. The toy example here takes in ~120k tokens, more than the largest prompt sizes.\n","- It's learning new information, not just trying to make sense of it given what it already learned (retrieval-augmented generation, or RAG). The best models use both RAG and finetuning together, which can be done easily with Lamini.\n","\n","\n","Here, you'll also explore ways to customize an LLM to your data, across different use cases, including but not limited to:\n","* Chatting / answering questions\n","* Scoring customer support conversations\n","* Extracting values from HTML forms\n","* Querying over code/engineering docs\n","* Reasoning and routing agents\n","* Writing articles\n","* Summarizing content and suggesting actions, e.g. from meeting transcripts\n","* Searching content, e.g. from Google docs\n","* Recommending content, e.g. health recs from patient EMR data"]},{"cell_type":"markdown","metadata":{"id":"MJqbS0TYd7N5"},"source":["# Setup ğŸ› ï¸\n","#### Note: You will be asked to sign in with Google, connected to your Lamini account.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19200,"status":"ok","timestamp":1701676533947,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"IZ432VBKOMGV","outputId":"77df9df7-95d7-45a6-8e86-bcc39b9286c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["<Response [200]>\n"]}],"source":["# @title Step 1: Authenticate with Google\n","\n","from google.colab import auth\n","import requests\n","import os\n","import yaml\n","\n","def authenticate_powerml():\n","  auth.authenticate_user()\n","  gcloud_token = !gcloud auth print-access-token\n","  powerml_token_response = requests.get('https://api.powerml.co/v1/auth/verify_gcloud_token?token=' + gcloud_token[0])\n","  print(powerml_token_response)\n","  return powerml_token_response.json()['token']\n","\n","key = authenticate_powerml()\n","\n","config = {\n","    \"production\": {\n","        \"key\": key,\n","        \"url\": \"https://api.powerml.co\"\n","    }\n","}\n","\n","keys_dir_path = '/root/.powerml'\n","os.makedirs(keys_dir_path, exist_ok=True)\n","\n","keys_file_path = keys_dir_path + '/configure_llama.yaml'\n","with open(keys_file_path, 'w') as f:\n","  yaml.dump(config, f, default_flow_style=False)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31147,"status":"ok","timestamp":1701676565080,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"lMkskdHX_W0n","outputId":"4b6e26d6-faf9-4c98-b788-ba1b6fcb0e85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on CoLab\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n","google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.3 which is incompatible.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# @title Step 2: Install the open-source [Lamini library](https://pypi.org/project/lamini/) to use LLMs easily\n","\n","#===> Run this block, when using the Google Colab. Otherwise, do not run it.\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on CoLab')\n","  # Install the package\n","  !pip install --upgrade --force-reinstall --ignore-installed -qqq lamini\n","else:\n","  print('Not running on CoLab')"]},{"cell_type":"markdown","metadata":{"id":"YmqDMAoGlTJR"},"source":["# ğŸš¨ Note: After installing, go to \"Runtime\" menu bar and then click on \"Restart session\" button, When the end of the installation package. Further, then go onto the next cell.\n","\n","# ğŸš¨ Lamini is just on a more recent version of numpy than Colab."]},{"cell_type":"markdown","metadata":{"id":"l1WGyMDZVcBJ"},"source":["# Build & Run ğŸƒâ€â™€ï¸"]},{"cell_type":"markdown","metadata":{"id":"rfNVJXKDSdlT"},"source":["Set up your LLM interface, with expected input and output types.\n","* These are based on standard Pydantic types.\n","* In this simple example, your input type is a question and the output type is an answer.\n","* Note: There's a Context field. This can be lightly prompt engineered.\n","* We include some other common types at the end of this notebook."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1358,"status":"ok","timestamp":1701676611073,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"_0c-WFKQT8QR"},"outputs":[],"source":["from llama import Type, Context\n","\n","class Input(Type):\n","    question: str = Context(\"question\")\n","\n","class Output(Type):\n","    answer: str = Context(\"answer to question\")\n"]},{"cell_type":"markdown","metadata":{"id":"G_10ZsvdTBI4"},"source":["Next, it's model time:\n","* Instantiate a model with the `InputOutputRunner` class.\n","* Specify which `input_type` and `output_type` you want to use.\n","* `model_name` is the base model you'll use. This one works for the free-tier. Please [contact us](https://www.lamini.ai/contact) for larger models."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":376,"status":"ok","timestamp":1701676613175,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"TYzgeYkANK5i"},"outputs":[],"source":["from llama import InputOutputRunner\n","\n","llm = InputOutputRunner(input_type=Input, output_type=Output, model_name=\"meta-llama/Llama-2-7b-chat-hf\")"]},{"cell_type":"markdown","metadata":{"id":"Q-EjBo7PNzyQ"},"source":["Then, it's data time:\n","* The `load_data_from_jsonlines` method takes a jsonline filepath and loads it into the model. More ways to load data from different files and basic python types in the [Lamini library docs](https://lamini-ai.github.io/runners/input_output_runner/).\n","* Set `verbose=True` to see if your data was loaded correctly into the input and output types, as well as how much data was added â€” always a good sanity check :)\n","* We recommend ~100-1000 examples to see some improvement in the base model here."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1380,"status":"ok","timestamp":1701676616956,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"qrR0RRc5QhaF"},"outputs":[],"source":["!wget -q -O \"lamini_docs.jsonl\" \"https://drive.google.com/uc?export=download&id=1rJDDI2wvEL4npvtOUaJ_-1zuCjBgSxHw\""]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701676618262,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"6NiIBgFbyPmC"},"outputs":[],"source":["# This code reads a JSONL file line by line, keeps the first 10 rows (or fewer if there are fewer than 10 rows in the file), and writes them to a new file.\n","\n","import json\n","\n","def read_and_process_jsonl(input_file, output_file, keep_rows=10):\n","    with open(input_file, 'r') as input_file:\n","        lines = input_file.readlines()\n","\n","    # Ensure not to exceed the total number of rows in the file\n","    keep_rows = min(keep_rows, len(lines))\n","\n","    # Keep the first 'keep_rows' rows\n","    selected_rows = lines[:keep_rows]\n","\n","    with open(output_file, 'w') as output_file:\n","        output_file.writelines(selected_rows)\n","\n","# Replace 'input.jsonl' and 'output.jsonl' with your actual file names\n","read_and_process_jsonl('lamini_docs.jsonl', 'lamini_docs_output.jsonl', keep_rows=10)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1701676621582,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"lV2JmQ-kNJIk","outputId":"5894818f-8b2c-42e4-fb91-d4e7bc105aff"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'question': \"What are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\"} {'answer': 'Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.'}\n","{'question': 'What is the recommended way to set up and configure the code repository?'} {'answer': 'Lamini can be downloaded as a python package and used in any codebase that uses python. Additionally, we provide a language agnostic REST API. Weâ€™ve seen users develop and train models in a notebook environment, and then switch over to a REST API to integrate with their production environment.'}\n","{'question': 'How can I find the specific documentation I need for a particular feature or function?'} {'answer': 'You can ask this model about documentation, which is trained on our publicly available docs and source code, or you can go to https://lamini-ai.github.io/.'}\n","{'question': \"Does the documentation include explanations of the code's purpose and how it fits into a larger system?\"} {'answer': 'Our documentation provides both real-world and toy examples of how one might use Lamini in a larger system. In particular, we have a walkthrough of how to build a Question Answer model available here: https://lamini-ai.github.io/example/'}\n","{'question': 'Does the documentation provide information about any external dependencies or libraries used by the code?'} {'answer': 'External dependencies and libraries are all available on the Python package hosting website Pypi at https://pypi.org/project/lamini/'}\n","{'question': 'How frequently is the documentation updated to reflect changes in the code?'} {'answer': 'Documentation on such a fast moving project is difficult to update regularly - thatâ€™s why weâ€™ve built this model to continually update users on the status of our product.'}\n","{'question': 'Is there a community or support channel mentioned in the documentation where I can ask questions or seek help?'} {'answer': 'You can always reach out to us at support@lamini.ai.'}\n","{'question': 'Are there any API references or documentation available for the codebase?'} {'answer': 'All our public documentation is available here https://lamini-ai.github.io/'}\n","{'question': 'Is there a troubleshooting guide or a list of common issues and their solutions?'} {'answer': 'All our public documentation is available here https://lamini-ai.github.io/'}\n","{'question': 'Are there any examples or sample code provided in the documentation?'} {'answer': 'Examples and sample documentation is available at https://lamini-ai.github.io/. In particular, there is a QA example where we show you how to feed your documentation into a model to ask questions about a code base. Additionally, sample code and colab notebooks are provided and linked throughout the documentation where relevant. Feedback on our documentation is greatly appreciated - we care about making LLMs - and by extension Lamini - easier to use. Please direct any feedback to support@lamini.ai.'}\n","Sample added data: [Input(question=\"What are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\"), Output(answer='Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.')]\n","Loaded 10 data pairs\n","Total data pairs: 10\n"]}],"source":["llm.load_data_from_jsonlines(\"lamini_docs_output.jsonl\", verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"jp9Nnn-KPJ9L"},"source":["The moment you've been waiting for: training!\n","* This will take ~10-15min (after it's thru the free-tier queue).\n","* Keep this cell running.\n","* You can see your model status on your [dashboard](https://app.lamini.ai/train).\n","* The run is private by default. Setting `is_public` to True makes the run shareable and public."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1011391,"status":"ok","timestamp":1701677635449,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"yOz398HxNSWo","outputId":"5fa8a629-c169-4c5d-fce2-e9b3a7f39128"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training job submitted! Check status of job 4376 here: https://app.lamini.ai/train/4376\n","Finetuning process completed, model name is: 5a19a1c990c656331eb4d364532cc477b2693dc5c706090bed5176503e1e95f2\n"]}],"source":["llm.train(is_public=True)"]},{"cell_type":"markdown","metadata":{"id":"2tff5G2NPPzq"},"source":["Finally, check out your model's results to compare the finetuned model to the base model's results.\n","\n","In code here, or on your [dashboard](https://app.lamini.ai/train).\n","\n","Please note that the dashboard results are not parsed, so you can see (and debug) the prompts visibly and easily."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":676,"status":"ok","timestamp":1701679893349,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"Gv3VDhj9NTFg","outputId":"424ee7e8-d860-4b94-feb3-4005d856cdde"},"outputs":[{"data":{"text/plain":["{'job_id': 4376,\n"," 'eval_results': [{'input': \"Task Definition:\\nGiven: question\\nGenerate: answer\\n\\nTask:\\nGiven: question: What are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\\n\\nGenerate: answer\\nanswer: \",\n","   'outputs': [{'model_name': '5a19a1c990c656331eb4d364532cc477b2693dc5c706090bed5176503e1e95f2',\n","     'output': ' Task Definition:\\nGiven: question\\nGenerate: answer'},\n","    {'model_name': 'Base model (meta-llama/Llama-2-7b-chat-hf)',\n","     'output': \"\\nThere are several types of documents available in the repository, including:\\n\\n* Installation guide: This document provides step-by-step instructions for installing and configuring the repository.\\n* API documentation: This document provides detailed information about the repository's API, including its endpoints, parameters, and response formats.\\n* Developer's guide: This document provides detailed information about developing applications that interact with the repository, including best practices and common use cases.\\n\\nNote: The answer is a list of three types of documents available in the repository, each with a brief description.\"}]},\n","  {'input': 'Task Definition:\\nGiven: question\\nGenerate: answer\\n\\nTask:\\nGiven: question: What is the recommended way to set up and configure the code repository?\\n\\nGenerate: answer\\nanswer: ',\n","   'outputs': [{'model_name': '5a19a1c990c656331eb4d364532cc477b2693dc5c706090bed5176503e1e95f2',\n","     'output': ' The recommended way to set up and configure the code repository is to use a version control system such as Git and follow best practices for repository layout and organization.'},\n","    {'model_name': 'Base model (meta-llama/Llama-2-7b-chat-hf)',\n","     'output': \"\\nThe recommended way to set up and configure the code repository depends on the specific needs and requirements of the project. However, here are some general best practices that can be followed:\\n\\n1. Use a version control system: A version control system such as Git or Mercurial should be used to manage the code repository. This will allow team members to collaborate on the codebase and track changes over time.\\n2. Create a separate repository for the code: It is recommended to have a separate repository for the codebase, rather than storing it in the same repository as the project's data and assets. This will make it easier to manage and maintain the codebase over time.\\n3. Use a consistent naming convention: Use a consistent naming convention for files and directories in the code repository. This will make it easier to navigate and understand the codebase.\\n4. Organize the codebase into logical directories: Organize the codebase into logical directories that reflect the structure of the project. This will make it easier to find and maintain the code.\\n5. Use a consistent formatting style: Use a consistent formatting style throughout the codebase. This will make it easier to read and understand the code.\\n6. Document the code: It is important\"}]}]}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["llm.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"Vg1-xQBGidBT"},"source":["## Congratulations, you've finetuned an LLM ğŸ‰\n","\n","* As you can see, the base model is really off the rails.\n","* Meanwhile, finetuning got the LLM to answer questions about new Lamini information correctly and coherently!\n","\n","Thanks for the tiny LLM, I'm ready for the real deal ğŸ’ª\n","\n","* If you want to build larger LLMs, run this live in production, host this on your own infrastructure (e.g. VPC or on premise), or other enterprise features, [please contact us](https://www.lamini.ai/contact)."]},{"cell_type":"markdown","metadata":{"id":"AHbS8XM2P8gE"},"source":["## Inference: Running the LLM after it's trained"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60771,"status":"ok","timestamp":1701679959748,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"HEqXIvtiGXuY","outputId":"7ccf9285-e858-43cd-b5ea-4f4d3ef4fdc3"},"outputs":[{"data":{"text/plain":["Output(answer=' To add data, you can follow these steps: Step 1: Determine the type of data you want to add and')"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["new_input = Input(question=\"How do I add data? Please help\")\n","llm(new_input)"]},{"cell_type":"markdown","metadata":{"id":"AXIGypzYbE7v"},"source":["Note that the output is in the Output type you specified."]},{"cell_type":"markdown","metadata":{"id":"DsszzyP8GhrH"},"source":["Use the model later, by instantiating it like this."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":651,"status":"ok","timestamp":1701679966422,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"vA28gCi50xk6"},"outputs":[],"source":["model_name = \"5a19a1c990c656331eb4d364532cc477b2693dc5c706090bed5176503e1e95f2\" # Or your model ID here"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701679967025,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"JDCq9N0mGjDI"},"outputs":[],"source":["llm_later = InputOutputRunner(input_type=Input, output_type=Output, model_name=model_name)"]},{"cell_type":"markdown","metadata":{"id":"8C6jlLQyNgD_"},"source":["## Customize inputs and outputs to your heart's desire!\n","\n","Here are some other common examples that we've seen:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwyIZKynNfYx"},"outputs":[],"source":["#######################################\n","# Score customer support conversations\n","#######################################\n","class InputConversation(Type):\n","    conversation: str = Context(\"conversation turns for customer support\")\n","\n","class OutputScore(Type):\n","    reason_for_score: str = Context(\"think step by step on how to score the customer conversation\")\n","    score: int = Context(\"score of 0 (bad, customer is unhappy), 1 (good, customer is happy with results), or -1 (unsure, unclear resolution)\")\n","\n","\n","#######################################\n","# Extract HTML elements\n","#######################################\n","class InputHTMLDOM(Type):\n","    html_dom: str = Context(\"html dom of a form page with first name, last name, and credit card number fields filled out\")\n","\n","class OutputForm(Type):\n","    first_name: str = Context(\"first name of the person\")\n","    last_name: str = Context(\"last name of the person\")\n","    credit_card_number: str = Context(\"credit card number of the person\")\n","\n","\n","#######################################\n","# Code/engineering docs\n","#######################################\n","class FunctionQuestion(Type):\n","    function: str = Context(\"function to ask about\")\n","\n","class FunctionAnswer(Type):\n","    inputs: list = Context(\"list of inputs to the function\")\n","    outputs: list = Context(\"list of outputs from the function\")\n","    description: str = Context(\"function description in 2 to 5 lines\")\n","\n","\n","#######################################\n","# Reasoning and routing agent\n","#######################################\n","class UserQuery(Type):\n","    query: str = Context(\"user's query\")\n","\n","class AgentAction(Type):\n","    thinking_steps: list = Context(\"think step by step about what action the agent needs to take\")\n","    action: str = Context(\"action to take\")\n","    backup_action: str = Context(\"fallback action to take\")\n","\n","\n","#######################################\n","# Article writing\n","#######################################\n","class Topic(Type):\n","    topic: str = Context(\"what topic the article should cover\")\n","\n","class Article(Type):\n","    title: str = Context(\"article title starting, written in markdown, e.g. with #\")\n","    article: str = Context(\"article body text, written in markdown\")\n","\n","\n","#######################################\n","# Content summarization\n","#######################################\n","class Transcript(Type):\n","    meeting_transcript: str = Context(\"transcript of the meeting (may contain some transcription errors)\")\n","\n","class McKinseySummary(Type):\n","    summary: str = Context(\"content summary in 3 bullet points\")\n","    action_items: list = Context(\"next action items to take\")\n","\n","\n","#######################################\n","# Search\n","#######################################\n","# Supporting documents\n","class Documents(Type):\n","    documents: list = Context(\"google docs dump\")\n","\n","# Input\n","class UserQuery(Type):\n","    search_query: str = Context(\"user's query\")\n","\n","# Output\n","class Result(Type):\n","    keywords: list = Context(\"keywords from the search query and results\")\n","    results: list = Context(\"top 3 search results\")\n","    reference_docs: list = Context(\"reference ID of the documents retrieved to support this search query\") # likely this is done another way for precision, but this is a start\n","\n","\n","#######################################\n","# Recommendation\n","#######################################\n","class EMR(Type):\n","    latest_hospital_visit: str = Context(\"patient's most recent hospital visit\")\n","\n","class Recommendation(Type):\n","    health_recommendations: list = Context(\"health recommendations\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1mxuzGfXc_S1fUSQV4AH1Qeao_biythhx","timestamp":1701675480929},{"file_id":"1w3IqPxA8FS8sim2XXTb9uIeqV1zToQs_","timestamp":1691465499848},{"file_id":"1IqhTK2NEy1dm4JWi6EG2wpPpvAnEQya5","timestamp":1691431676828},{"file_id":"1cn5AaWAsn6oOjz8bbo5JN9m893vmguCh","timestamp":1690414380362},{"file_id":"1QMeGzR9FnhNJJFmcHtm9RhFP3vrwIkFn","timestamp":1689803150824}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
