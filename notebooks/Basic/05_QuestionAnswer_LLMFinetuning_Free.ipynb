{"cells":[{"cell_type":"markdown","metadata":{"id":"S-y0hVsPe3rG"},"source":["<a href=\"https://colab.research.google.com/github/datafyresearcher/datafy-finetuning-beginner/blob/main/notebooks/Basic/05_QuestionAnswer_LLMFinetuning_Free.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"4dIzkKjG1f_R"},"source":["# Finetune a question-answer LLM over your data using [Lamini](https://www.lamini.ai/)\n","\n","- Prepare question-answer pairs\n","- Load it into the LLM\n","- Finetune the LLM on it within 15 minutes.\n","\n","It's completely free! What's special is that the LLM is learning not only how to answer questions, but also new up-to-date information that the general LLMs aren't away of.\n","\n","We include some question-answer datasets for you to finetune:\n","- Lamini engineering docs\n","- Taylor Swift recent facts\n","- Open-Source LLMs\n","- BTS recent facts"]},{"cell_type":"markdown","metadata":{"id":"MJqbS0TYd7N5"},"source":["# Setup 🛠️\n","### Note: You will be asked to sign in with Google, connected to your Lamini account.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18821,"status":"ok","timestamp":1701670886463,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"IZ432VBKOMGV","outputId":"b0ebfe80-d9bb-4f85-d4a0-a28c23aa8662"},"outputs":[{"name":"stdout","output_type":"stream","text":["<Response [200]>\n"]}],"source":["# @title Step 1: Authenticate with Google\n","\n","from google.colab import auth\n","import requests\n","import os\n","import yaml\n","\n","def authenticate_powerml():\n","  auth.authenticate_user()\n","  gcloud_token = !gcloud auth print-access-token\n","  powerml_token_response = requests.get('https://api.powerml.co/v1/auth/verify_gcloud_token?token=' + gcloud_token[0])\n","  print(powerml_token_response)\n","  return powerml_token_response.json()['token']\n","\n","key = authenticate_powerml()\n","\n","config = {\n","    \"production\": {\n","        \"key\": key,\n","        \"url\": \"https://api.powerml.co\"\n","    }\n","}\n","\n","keys_dir_path = '/root/.powerml'\n","os.makedirs(keys_dir_path, exist_ok=True)\n","\n","keys_file_path = keys_dir_path + '/configure_llama.yaml'\n","with open(keys_file_path, 'w') as f:\n","  yaml.dump(config, f, default_flow_style=False)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31065,"status":"ok","timestamp":1701670976386,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"lMkskdHX_W0n","outputId":"d8577ade-8127-4941-ae26-364a13b8778b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on CoLab\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n","google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.3 which is incompatible.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# @title Step 2: Install the open-source [Lamini library](https://pypi.org/project/lamini/) to use LLMs easily\n","\n","#===> Run this block, when using the Google Colab. Otherwise, do not run it.\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on CoLab')\n","  # Install the package\n","  !pip install --upgrade --force-reinstall --ignore-installed -qqq lamini\n","else:\n","  print('Not running on CoLab')\n"]},{"cell_type":"markdown","metadata":{"id":"jQsMKWK_l70w"},"source":["# 🚨 Note: After installing, go to \"Runtime\" menu bar and then click on \"Restart session\" button, When the end of the installation package. Further, then go onto the next cell.\n","\n","# 🚨 Lamini is just on a more recent version of numpy than Colab.\n"]},{"cell_type":"markdown","metadata":{"id":"LLmpQtZ82TXQ"},"source":["# Prepare your data 📊\n","\n","Upload your question-answer data in the following format (jsonl):\n","```\n","{\"question\": \"type your question\", \"answer\": \"answer to the question\"}\n","\n","```\n","Upload your question-answer data in the following format (csv):\n","```\n","Make sure that you have 'question' and 'answer' as column keys\n","\n","```\n","You can also download a sample `seed_lamini_docs.jsonl` file, with Lamini question-answer data in it 🦙\n","\n","Also we have some more example related to Taylor Swift &nbsp;👑, BTS &nbsp;💜, and Open LLMs &nbsp;📚, try it out!"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5506,"status":"ok","timestamp":1701671009700,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"uNC7eRO5eeXl"},"outputs":[],"source":["!wget -q -O \"seed_lamini_docs.jsonl\" \"https://drive.google.com/uc?export=download&id=1SfGp1tVuLTs0WYDugZcxX-EHrmDtYrYJ\"\n","!wget -q -O \"seed_taylor_swift.jsonl\" \"https://drive.google.com/uc?export=download&id=119sHYYImcXEbGyvS3wWGpkSEVIFdLy6Z\"\n","!wget -q -O \"seed_bts.csv\" \"https://drive.google.com/uc?export=download&id=1lblhdhKwoiOjlvfk8tr7Ieo4KpvjRm6n\"\n","!wget -q -O \"seed_open_llm.jsonl\" \"https://drive.google.com/uc?export=download&id=1S7oPPko-UmOr-bqkZ_PREfGKO2f73ZiK\""]},{"cell_type":"markdown","metadata":{"id":"Pbw54_Nw3TzL"},"source":["# **Finetune your LLM 🦙**\n","\n","Finetuning has a simple interface. The basic premise is:\n"]},{"cell_type":"markdown","metadata":{"id":"lP8OIFWpgTLh"},"source":["## 1. Instantiate the LLM\n","\n","To use different models for finetuning, you can pass in model_name parameter to QuestionAnswerModel(), for example:\n","```\n","  model = QuestionAnswerModel(model_name=\"YOUR_MODEL_NAME\")\n","```\n","Currently the free tier version supports limited models, you can find the list [here](https://lamini-ai.github.io/notebooks/#lamini-finetuning-for-free)."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1701674181939,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"9Nbqsj2OOnMr"},"outputs":[],"source":["from llama import QuestionAnswerModel\n","import time\n","\n","# Instantiate the model and load the data into it\n","finetune_model = QuestionAnswerModel()"]},{"cell_type":"markdown","metadata":{"id":"ZR4JvrTfgauE"},"source":["## 2. Load your data into the LLM"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":551,"status":"ok","timestamp":1701674201327,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"F2WWHSmJr3Uh"},"outputs":[],"source":["# This code reads a JSONL file line by line, keeps the first 10 rows (or fewer if there are fewer than 10 rows in the file), and writes them to a new file.\n","\n","import json\n","\n","def read_and_process_jsonl(input_file, output_file, keep_rows=10):\n","    with open(input_file, 'r') as input_file:\n","        lines = input_file.readlines()\n","\n","    # Ensure not to exceed the total number of rows in the file\n","    keep_rows = min(keep_rows, len(lines))\n","\n","    # Keep the first 'keep_rows' rows\n","    selected_rows = lines[:keep_rows]\n","\n","    with open(output_file, 'w') as output_file:\n","        output_file.writelines(selected_rows)\n","\n","# Replace 'input.jsonl' and 'output.jsonl' with your actual file names\n","read_and_process_jsonl('seed_lamini_docs.jsonl', 'seed_lamini_docs_output.jsonl', keep_rows=10)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701674228519,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"amOBaw8ZgWM_"},"outputs":[],"source":["finetune_model.load_question_answer_from_jsonlines(\"seed_lamini_docs_output.jsonl\")\n","# OR\n","# model.load_question_answer_from_csv(\"seed_bts.csv\")"]},{"cell_type":"markdown","metadata":{"id":"tVoAfNZvg4Xs"},"source":["## 3. Train the LLM\n","\n","Once the model finishes training, you can view its responses, chat, and compare it to the base model on https://app.lamini.ai/train 👈"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283237,"status":"ok","timestamp":1701674523454,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"0YxBlP5Yrv1_","outputId":"6eee3588-561a-436f-c9fe-aa740596462b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training job submitted! Check status of job 4374 here: https://app.lamini.ai/train/4374\n","Finetuning process completed, model name is: 646526d469641d43640692e6399c8b312eebd57d88bcad6ad39a2393e373790a\n","Time taken: 283.47936820983887 seconds\n"]}],"source":["# Train the model (4:30 minutes)\n","start=time.time()\n","finetune_model.train() # enable_peft=True\n","print(f\"Time taken: {time.time()-start} seconds\")"]},{"cell_type":"markdown","metadata":{"id":"c2KhOQ1gg9V8"},"source":["## 4. Compare your LLM: before and after training (optional)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1701674920346,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"UUo9HX0lOw1S"},"outputs":[],"source":["# Functions for printing results during training...\n","def print_training_results(results):\n","    print(\"-\"*100)\n","    print(\"Training Results\")\n","    print(results)\n","    print(\"-\"*100)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1358,"status":"ok","timestamp":1701674922496,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"6qUD4s22rnK7","outputId":"d2c679e4-2cc5-414d-d864-a4579a3cfa7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------------------------------------------\n","Training Results\n","{'job_id': 4374, 'eval_results': [{'input': \"What are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\", 'outputs': [{'model_name': '646526d469641d43640692e6399c8b312eebd57d88bcad6ad39a2393e373790a', 'output': ' All of these are available in the documentation.\\n\\nHow can I find the specific documentation I need for a particular feature or function? You can ask this model about documentation, which is trained on our publicly available docs and source code, or you can go to https://lamini-ai.github.io/.\\n\\nHow frequently is the documentation updated to reflect changes in the code? Documentation on such a fast moving project is difficult to update regularly - that’s why we’ve built this model to continually update users on the status of our product.\\n\\nDoes the documentation provide information about any external dependencies or libraries used by the code? External dependencies and libraries are all available on the Python package hosting website Pypi at https://pypi.org/project/lamini-ai/lamini-ai/latest/\\n\\nDoes the documentation provide information about any external dependencies or libraries used by the code? External dependencies and libraries are all available on the Python package hosting website Pypi at https://pypi.org/project/lamini-ai/lamini-ai/latest/How frequently is the documentation updated to reflect changes in the code? Documentation on such a fast moving project is difficult to update regularly - that'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': \"\\n\\nA:\\n\\nThe documentation is available in the repository.\\n\\nA:\\n\\nThe documentation is available in the repository.\\n\\nA:\\n\\nThe documentation is available in the repository.\\n\\nA:\\n\\nThe documentation is available in the repository.\\n\\nQ:\\n\\nHow to get the value of a variable in a function?\\n\\nI have a function that takes a variable and returns a value.\\nfunction get_value(var_name) {\\n    return var_name;\\n}\\n\\nI want to get the value of var_name in the function.\\nI tried this:\\nfunction get_value(var_name) {\\n    return var_name;\\n}\\n\\nvar_name = get_value('var_name');\\n\\nBut it doesn't work.\\n\\nA:\\n\\nYou can use the function's return value:\\nfunction get_value(var_name) {\\n    return var_name;\\n}\\n\\nvar_name = get_value('var_name');\\n\\nconsole.log(var_name); // var_name\\n\\nA:\\n\\nYou can use the return value of the function:\\nfunction get_value(\"}]}, {'input': 'What is the recommended way to set up and configure the code repository?', 'outputs': [{'model_name': '646526d469641d43640692e6399c8b312eebd57d88bcad6ad39a2393e373790a', 'output': ' All our public documentation is available here https://lamini-ai.github.io/'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': '\\n\\nA:\\n\\nThe recommended way to set up and configure the code repository is to use the code repository in the project.\\n\\nYou can use the code repository in the project.\\n\\nThe code repository is a repository for the code you have written.\\n\\nYou can use the code repository in the project.\\n\\nThe code repository is a repository for the code you have written.\\n\\nYou can use the code repository in the project.\\n\\nThe code repository is a repository for the code you have written.\\n\\nYou can use the code repository in the project.\\n\\nThe code repository is a repository for the code you have written.\\n\\nYou can use the code repository in the project.\\n\\nThe code repository is a repository for the code you have written.\\n\\nYou can use the code repository in the project.\\n\\nThe code repository is a repository for the code you have written.\\n\\nYou can use the code repository in the project.\\n\\nThe code repository is a repository for the code you have written.\\n\\nYou can use the code repository in the project.\\n\\nThe code repository is a repository for the code you have written.\\n\\nYou can use the code repository in the project.\\n'}]}]}\n","----------------------------------------------------------------------------------------------------\n"]}],"source":["# Evaluate base and finetuned models to compare performance\n","results = finetune_model.get_eval_results()\n","print_training_results(results)"]},{"cell_type":"markdown","metadata":{"id":"6aEFv9jJhZQA"},"source":["## 5. Run your trained LLM"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":33711,"status":"ok","timestamp":1701674624477,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"vvVtP_LGg-ZF","outputId":"942d88d8-129c-420b-b539-9110f4fba481"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n\\nI have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form. I have a model called Lamini that I am using to build a form'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["answer = finetune_model.get_answer(\"How can I add data to Lamini?\")\n","answer"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1760,"status":"ok","timestamp":1701674759613,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"DOhMqPbgtoLD","outputId":"61faf931-e431-4e29-c8e7-230fd3fe210c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' All our public documentation is available here https://lamini-ai.github.io/'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["answer = finetune_model.get_answer(\"How frequently is the documentation updated to reflect changes in the code?\")\n","answer"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":11992,"status":"ok","timestamp":1701674862608,"user":{"displayName":"Datafy Research","userId":"04455658766472393665"},"user_tz":-300},"id":"rNiCAm-ruGfs","outputId":"7ea6ea35-09a4-4eda-a374-842a12fd188a"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\" Or is there a list of all the documentation available for a particular feature or function? Or is there a list of all the documentation available for a particular function? - soren\\n\\nHi, I'm looking for the documentation for a particular feature or function. I've looked on the web and on the docs, but can't find the specific documentation I need for a particular feature or function? Or is there a list of all the documentation available for a particular feature or function? Or is there a list of all the documentation available for a particular function? - soren\\n\\nHi, I'm looking for the documentation for a particular feature or function. I've looked on the web and on the docs, but can't find the specific documentation I need for a particular feature or function? Or is there a list of all the documentation available for a particular function? Or is there a list of all the documentation available for a particular function? - soren\\n\\nHi, I'm looking for the documentation for a particular feature or function. I've looked on the web and on the docs, but can't find the specific documentation I need for a particular feature or function? Or is there a list of all the documentation available for a particular function? Or is there a list of\""]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["answer = finetune_model.get_answer(\"How can I find the specific documentation I need for a particular feature or function?\")\n","answer"]},{"cell_type":"markdown","metadata":{"id":"Vg1-xQBGidBT"},"source":["## Congratulations, you've finetuned an LLM 🎉\n","\n","As you can see, the base model is really off the rails. Meanwhile, finetuning got the LLM to answer the question correctly and coherently!"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1S7VfBqaxM4kMJriqo8yb0QnfgI0IMYsp","timestamp":1701670639621},{"file_id":"1QMeGzR9FnhNJJFmcHtm9RhFP3vrwIkFn","timestamp":1692941444595}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
