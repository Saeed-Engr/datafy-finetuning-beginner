{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/datafyresearcher/datafy-finetuning-beginner/blob/main/notebooks/Basic/03_Training_lab_student_lamini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ivsk35dZnnO"},"outputs":[],"source":["#===> Run this block, when using the Google Colab. Otherwise, do not run it.\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on CoLab')\n","  # Install the package\n","  ! pip install datasets jsonlines python-dotenv lamini transformers[torch] -q\n","else:\n","  print('Not running on CoLab')"]},{"cell_type":"markdown","metadata":{"id":"hSsW_xQ8ZiqZ"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"ozJEJi6d06SG","tags":[]},"source":["## Technically, it's only a few lines of code to run on GPUs (elsewhere, ie. on Lamini).\n","```\n","from llama import BasicModelRunner\n","\n","model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n","model.load_data_from_jsonlines(\"lamini_docs.jsonl\")\n","model.train()\n","```\n","1. Choose base model.\n","2. Load data.\n","3. Train it. Returns a model ID, dashboard, and playground interface.\n","\n","### Let's look under the hood at the core code running this! This is the open core of Lamini's `llama` library :)"]},{"cell_type":"code","execution_count":6,"metadata":{"height":388,"id":"VOFTTn10Ziqd"},"outputs":[],"source":["import datasets\n","import logging\n","import torch\n","\n","from utilities import *\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForCausalLM\n","from transformers import TrainingArguments\n","from transformers import AutoModelForCausalLM\n","from llama import BasicModelRunner\n","\n","logger = logging.getLogger(__name__)\n","global_config = None"]},{"cell_type":"markdown","metadata":{"id":"dkzE-rhpZiqe"},"source":["### Load the Lamini docs dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"height":65,"id":"tA4UsuflZiqe"},"outputs":[],"source":["dataset_name = \"lamini_docs.jsonl\"\n","dataset_path = f\"/content/{dataset_name}\"\n","use_hf = False"]},{"cell_type":"code","execution_count":null,"metadata":{"height":48,"id":"gMxpvvgcZiqf"},"outputs":[],"source":["dataset_path = \"lamini/lamini_docs\"\n","use_hf = True"]},{"cell_type":"markdown","metadata":{"id":"vQPqcdAsZiqf"},"source":["### Set up the model, training config, and tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"height":31,"id":"9atPiryFZiqf"},"outputs":[],"source":["model_name = \"EleutherAI/pythia-70m\""]},{"cell_type":"code","execution_count":null,"metadata":{"height":201,"id":"eSqEpFZWZiqf"},"outputs":[],"source":["training_config = {\n","    \"model\": {\n","        \"pretrained_name\": model_name,\n","        \"max_length\" : 2048\n","    },\n","    \"datasets\": {\n","        \"use_hf\": use_hf,\n","        \"path\": dataset_path\n","    },\n","    \"verbose\": True\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"height":137,"id":"wL2_u_0BZiqg","scrolled":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n","\n","print(train_dataset)\n","print(test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"rr-hymjcZiqg"},"source":["### Load the base model"]},{"cell_type":"code","execution_count":null,"metadata":{"height":31,"id":"5p_9Q9w2Ziqg"},"outputs":[],"source":["base_model = AutoModelForCausalLM.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"height":133,"id":"5ECyc6kkZiqh"},"outputs":[],"source":["device_count = torch.cuda.device_count()\n","if device_count > 0:\n","    logger.debug(\"Select GPU device\")\n","    device = torch.device(\"cuda\")\n","else:\n","    logger.debug(\"Select CPU device\")\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"height":31,"id":"EzKVydhEZiqh"},"outputs":[],"source":["base_model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"DOiB2S4CZiqh","tags":[]},"source":["### Define function to carry out inference"]},{"cell_type":"code","execution_count":null,"metadata":{"height":426,"id":"iV-iofQ0Ziqh"},"outputs":[],"source":["def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n","  # Tokenize\n","  input_ids = tokenizer.encode(\n","          text,\n","          return_tensors=\"pt\",\n","          truncation=True,\n","          max_length=max_input_tokens\n","  )\n","\n","  # Generate\n","  device = model.device\n","  generated_tokens_with_prompt = model.generate(\n","    input_ids=input_ids.to(device),\n","    max_length=max_output_tokens\n","  )\n","\n","  # Decode\n","  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n","\n","  # Strip the prompt\n","  generated_text_answer = generated_text_with_prompt[0][len(text):]\n","\n","  return generated_text_answer"]},{"cell_type":"markdown","metadata":{"id":"GXzGyw6bZiqi"},"source":["### Try the base model"]},{"cell_type":"code","execution_count":null,"metadata":{"height":120,"id":"ZRLLEP0UZiqi"},"outputs":[],"source":["test_text = test_dataset[0]['question']\n","print(\"Question input (test):\", test_text)\n","print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n","print(\"Model's answer: \")\n","print(inference(test_text, base_model, tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"cgoTl0iRZiqi"},"source":["### Setup training"]},{"cell_type":"code","execution_count":null,"metadata":{"height":31,"id":"GYw-KZYfZiqi"},"outputs":[],"source":["max_steps = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"height":48,"id":"GDxhauc-Ziqi"},"outputs":[],"source":["trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n","output_dir = trained_model_name"]},{"cell_type":"code","execution_count":null,"metadata":{"height":681,"id":"gJNYC8OJZiqi"},"outputs":[],"source":["training_args = TrainingArguments(\n","\n","  # Learning rate\n","  learning_rate=1.0e-5,\n","\n","  # Number of training epochs\n","  num_train_epochs=1,\n","\n","  # Max steps to train for (each step is a batch of data)\n","  # Overrides num_train_epochs, if not -1\n","  max_steps=max_steps,\n","\n","  # Batch size for training\n","  per_device_train_batch_size=1,\n","\n","  # Directory to save model checkpoints\n","  output_dir=output_dir,\n","\n","  # Other arguments\n","  overwrite_output_dir=False, # Overwrite the content of the output directory\n","  disable_tqdm=False, # Disable progress bars\n","  eval_steps=120, # Number of update steps between two evaluations\n","  save_steps=120, # After # steps model is saved\n","  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n","  per_device_eval_batch_size=1, # Batch size for evaluation\n","  evaluation_strategy=\"steps\",\n","  logging_strategy=\"steps\",\n","  logging_steps=1,\n","  optim=\"adafactor\",\n","  gradient_accumulation_steps = 4,\n","  gradient_checkpointing=False,\n","\n","  # Parameters for early stopping\n","  load_best_model_at_end=True,\n","  save_total_limit=1,\n","  metric_for_best_model=\"eval_loss\",\n","  greater_is_better=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"height":273,"id":"v0BYzN9bZiqj"},"outputs":[],"source":["model_flops = (\n","  base_model.floating_point_ops(\n","    {\n","       \"input_ids\": torch.zeros(\n","           (1, training_config[\"model\"][\"max_length\"])\n","      )\n","    }\n","  )\n","  * training_args.gradient_accumulation_steps\n",")\n","\n","print(base_model)\n","print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n","print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"]},{"cell_type":"code","execution_count":null,"metadata":{"height":150,"id":"Tc5Xo8Z6Ziqj"},"outputs":[],"source":["trainer = Trainer(\n","    model=base_model,\n","    model_flops=model_flops,\n","    total_steps=max_steps,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"]},{"cell_type":"markdown","metadata":{"id":"xE6D-yrAZiqj"},"source":["### Train a few steps"]},{"cell_type":"code","execution_count":null,"metadata":{"height":31,"id":"L0pmrPwlZiqj"},"outputs":[],"source":["training_output = trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"kz3AA_FzZiqk"},"source":["### Save model locally"]},{"cell_type":"code","execution_count":null,"metadata":{"height":82,"id":"9Dj49RXnZiqk"},"outputs":[],"source":["save_dir = f'{output_dir}/final'\n","\n","trainer.save_model(save_dir)\n","print(\"Saved model to:\", save_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"height":69,"id":"kmahp80iZiqk"},"outputs":[],"source":["finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"height":48,"id":"0a6wFaycZiqk"},"outputs":[],"source":["finetuned_slightly_model.to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"WJwzqp6uZiqk"},"source":["### Run slightly trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"height":99,"id":"TdWBdWr7Ziqk"},"outputs":[],"source":["test_question = test_dataset[0]['question']\n","print(\"Question input (test):\", test_question)\n","\n","print(\"Finetuned slightly model's answer: \")\n","print(inference(test_question, finetuned_slightly_model, tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"height":48,"id":"8Dt3Oy8wZiql"},"outputs":[],"source":["test_answer = test_dataset[0]['answer']\n","print(\"Target answer output (test):\", test_answer)"]},{"cell_type":"markdown","metadata":{"id":"y3lUJVJxZiql"},"source":["### Run same model trained for two epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"height":137,"id":"PxWEBLW5Ziql"},"outputs":[],"source":["finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n","tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n","\n","finetuned_longer_model.to(device)\n","print(\"Finetuned longer model's answer: \")\n","print(inference(test_question, finetuned_longer_model, tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"ksT4oAGdZiql"},"source":["### Run much larger trained model and explore moderation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xw4LWTflZiql"},"outputs":[],"source":["# LAMINI_API_KEY is saved in `.env` file\n","\n","# !export LAMINI_API_KEY= \"<YOUR-KEY-HERE>\"\n","from dotenv import load_dotenv\n","import os\n","load_dotenv()\n","LAMINI_API_KEY=os.getenv(\"LAMINI_API_KEY\")\n","\n","from llama import LLMEngine\n","\n","llm = LLMEngine(\n","    id=\"marketing\",\n","    config={\n","        \"production\": {\n","            \"key\": LAMINI_API_KEY,\n","        }\n","    },\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"height":86,"id":"sS67rd_OZiql"},"outputs":[],"source":["bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n","bigger_finetuned_output = bigger_finetuned_model(test_question)\n","print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"height":137,"id":"5cFtz2ZOZiqm"},"outputs":[],"source":["count = 0\n","for i in range(len(train_dataset)):\n"," if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n","  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n","  count += 1\n","print(count)"]},{"cell_type":"markdown","metadata":{"id":"9a3bXm5BZiqn"},"source":["### Explore moderation using small model\n","First, try the non-finetuned base model:"]},{"cell_type":"code","execution_count":null,"metadata":{"height":86,"id":"tGg89-gJZiqn"},"outputs":[],"source":["base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n","base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n","print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"MmDJIIfkZiqn"},"source":["### Now try moderation with finetuned small model"]},{"cell_type":"code","execution_count":null,"metadata":{"height":52,"id":"Odn4ui1YZiqn"},"outputs":[],"source":["print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"xfMPjtkUZiqo"},"source":["### Finetune a model in 3 lines of code using Lamini"]},{"cell_type":"code","execution_count":null,"metadata":{"height":86,"id":"q3-qz-pPZiqo"},"outputs":[],"source":["model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n","model.load_data_from_jsonlines(\"lamini_docs.jsonl\")\n","model.train(is_public=True) # -> returns an ID, dashboard, and chat interface"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6824,"status":"ok","timestamp":1694423615660,"user":{"displayName":"Amjad Raza","userId":"06768084019046926999"},"user_tz":-600},"id":"4-8PDS-hg2xT","outputId":"3e25328c-0193-4662-d90d-7b4b2c127e60"},"outputs":[{"name":"stdout","output_type":"stream","text":["Refresh index: 100% (19/19), done.\n","On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   notebooks/01_Why_finetuning_lab_student.ipynb\u001b[m\n","\t\u001b[31mmodified:   notebooks/02_Where_finetuning_fits_in_lab_student.ipynb\u001b[m\n","\t\u001b[31mmodified:   notebooks/03_Instruction_tuning_lab_student.ipynb\u001b[m\n","\t\u001b[31mmodified:   notebooks/04_Data_preparation_lab_student.ipynb\u001b[m\n","\t\u001b[31mmodified:   notebooks/05_Training_lab_student.ipynb\u001b[m\n","\t\u001b[31mmodified:   notebooks/0_Preparing_setup_datafy_finetuning.ipynb\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31malpaca_processed.jsonl\u001b[m\n","\t\u001b[31mlamini_docs_3_steps/\u001b[m\n","\t\u001b[31mlamini_docs_processed.jsonl\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","\n","!git status"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1694423697713,"user":{"displayName":"Amjad Raza","userId":"06768084019046926999"},"user_tz":-600},"id":"bIh--ABdhKWf","outputId":"899ac083-10b7-4053-fa24-1199b6f9ec02"},"outputs":[{"name":"stdout","output_type":"stream","text":["Author identity unknown\n","\n","*** Please tell me who you are.\n","\n","Run\n","\n","  git config --global user.email \"you@example.com\"\n","  git config --global user.name \"Your Name\"\n","\n","to set your account's default identity.\n","Omit --global to set the identity only in this repository.\n","\n","fatal: unable to auto-detect email address (got 'root@c3678bff7eee.(none)')\n"]}],"source":["! git commit -m \"Added Code to run in Google Colab\""]},{"cell_type":"markdown","metadata":{"id":"JTWYtPalZiqo"},"source":["Follow the link above. The training can take a few minutes. You may need to refresh the URL to detect the change from 'In Progress' to 'Completed'"]},{"cell_type":"markdown","metadata":{"id":"BKEChLMyZiqo"},"source":["![image.png](attachment:5b0e47de-93a9-4f8e-8cd7-a42de1ce4fe3.png)"]},{"cell_type":"markdown","metadata":{"id":"ROrdxKbNZiqo"},"source":["![image.png](attachment:02381ae5-d062-4f87-b7fe-6ac45b7f4fdb.png)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
