{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/datafyresearcher/datafy-finetuning-beginner/blob/main/notebooks/Medium/03_Training_lab_student_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ivsk35dZnnO"},"outputs":[],"source":["#===> Run this block, when using the Google Colab. Otherwise, do not run it.\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on CoLab')\n","  # Install the package\n","  ! pip install transformers[torch] jsonlines python-dotenv -q\n","else:\n","  print('Not running on CoLab')"]},{"cell_type":"markdown","metadata":{"id":"hSsW_xQ8ZiqZ"},"source":["## Training"]},{"cell_type":"code","execution_count":2,"metadata":{"height":388,"id":"VOFTTn10Ziqd"},"outputs":[],"source":["import logging\n","import logging\n","import torch\n","\n","from utilities import *\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForCausalLM\n","from transformers import TrainingArguments\n","from transformers import AutoModelForCausalLM\n","\n","logger = logging.getLogger(__name__)\n","global_config = None"]},{"cell_type":"markdown","metadata":{"id":"dkzE-rhpZiqe"},"source":["### Load the Lamini docs dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"height":48,"id":"gMxpvvgcZiqf"},"outputs":[],"source":["dataset_path = \"lamini/lamini_docs\"\n","use_hf = True"]},{"cell_type":"markdown","metadata":{"id":"vQPqcdAsZiqf"},"source":["### Set up the model, training config, and tokenizer"]},{"cell_type":"code","execution_count":4,"metadata":{"height":31,"id":"9atPiryFZiqf"},"outputs":[],"source":["model_name = \"EleutherAI/pythia-70m\""]},{"cell_type":"code","execution_count":5,"metadata":{"height":201,"id":"eSqEpFZWZiqf"},"outputs":[],"source":["training_config = {\n","    \"model\": {\n","        \"pretrained_name\": model_name,\n","        \"max_length\" : 2048\n","    },\n","    \"datasets\": {\n","        \"use_hf\": use_hf,\n","        \"path\": dataset_path\n","    },\n","    \"verbose\": True\n","}"]},{"cell_type":"code","execution_count":6,"metadata":{"height":137,"id":"wL2_u_0BZiqg","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-08 11:30:19,158 - DEBUG - utilities - Config: datasets.path: lamini/lamini_docs\n","datasets.use_hf: true\n","model.max_length: 2048\n","model.pretrained_name: EleutherAI/pythia-70m\n","verbose: true\n","\n"]},{"name":"stdout","output_type":"stream","text":["tokenize True lamini/lamini_docs\n"]},{"name":"stderr","output_type":"stream","text":["2023-12-08 11:30:23,598 - DEBUG - fsspec.local - open file: /home/dafay/.cache/huggingface/datasets/lamini___lamini_docs/default-a15134f5c9ebe39e/0.0.0/e58c486e4bad3c9cf8d969f920449d1103bbdf069a7150db2cf96c695aeca990/dataset_info.json\n","2023-12-08 11:30:23,636 - DEBUG - fsspec.local - open file: /home/dafay/.cache/huggingface/datasets/lamini___lamini_docs/default-a15134f5c9ebe39e/0.0.0/e58c486e4bad3c9cf8d969f920449d1103bbdf069a7150db2cf96c695aeca990/dataset_info.json\n"]},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n","    num_rows: 1260\n","})\n","Dataset({\n","    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n","    num_rows: 140\n","})\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n","\n","print(train_dataset)\n","print(test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"rr-hymjcZiqg"},"source":["### Load the base model"]},{"cell_type":"code","execution_count":7,"metadata":{"height":31,"id":"5p_9Q9w2Ziqg"},"outputs":[],"source":["base_model = AutoModelForCausalLM.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":8,"metadata":{"height":133,"id":"5ECyc6kkZiqh"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-08 11:30:34,545 - DEBUG - __main__ - Select CPU device\n"]}],"source":["device_count = torch.cuda.device_count()\n","if device_count > 0:\n","    logger.debug(\"Select GPU device\")\n","    device = torch.device(\"cuda\")\n","else:\n","    logger.debug(\"Select CPU device\")\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":9,"metadata":{"height":31,"id":"EzKVydhEZiqh"},"outputs":[{"data":{"text/plain":["GPTNeoXForCausalLM(\n","  (gpt_neox): GPTNeoXModel(\n","    (embed_in): Embedding(50304, 512)\n","    (emb_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-5): 6 x GPTNeoXLayer(\n","        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n","        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n","        (attention): GPTNeoXAttention(\n","          (rotary_emb): GPTNeoXRotaryEmbedding()\n","          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n","          (dense): Linear(in_features=512, out_features=512, bias=True)\n","          (attention_dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (mlp): GPTNeoXMLP(\n","          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n","          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n","          (act): GELUActivation()\n","        )\n","      )\n","    )\n","    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["base_model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"DOiB2S4CZiqh","tags":[]},"source":["### Define function to carry out inference"]},{"cell_type":"code","execution_count":10,"metadata":{"height":426,"id":"iV-iofQ0Ziqh"},"outputs":[],"source":["def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n","  # Tokenize\n","  input_ids = tokenizer.encode(\n","          text,\n","          return_tensors=\"pt\",\n","          truncation=True,\n","          max_length=max_input_tokens\n","  )\n","\n","  # Generate\n","  device = model.device\n","  generated_tokens_with_prompt = model.generate(\n","    input_ids=input_ids.to(device),\n","    max_length=max_output_tokens\n","  )\n","\n","  # Decode\n","  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n","\n","  # Strip the prompt\n","  generated_text_answer = generated_text_with_prompt[0][len(text):]\n","\n","  return generated_text_answer"]},{"cell_type":"markdown","metadata":{"id":"GXzGyw6bZiqi"},"source":["### Try the base model"]},{"cell_type":"code","execution_count":11,"metadata":{"height":120,"id":"ZRLLEP0UZiqi"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n","Correct answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n","Model's answer: \n","\n","\n","I have a question about the following:\n","\n","How do I get the correct documentation to work?\n","\n","A:\n","\n","I think you need to use the following code:\n","\n","A:\n","\n","You can use the following code to get the correct documentation.\n","\n","A:\n","\n","You can use the following code to get the correct documentation.\n","\n","A:\n","\n","You can use the following\n"]}],"source":["test_text = test_dataset[0]['question']\n","print(\"Question input (test):\", test_text)\n","print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n","print(\"Model's answer: \")\n","print(inference(test_text, base_model, tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"cgoTl0iRZiqi"},"source":["### Setup training"]},{"cell_type":"code","execution_count":12,"metadata":{"height":31,"id":"GYw-KZYfZiqi"},"outputs":[],"source":["max_steps = 3"]},{"cell_type":"code","execution_count":13,"metadata":{"height":48,"id":"GDxhauc-Ziqi"},"outputs":[],"source":["trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n","output_dir = trained_model_name"]},{"cell_type":"code","execution_count":14,"metadata":{"height":681,"id":"gJNYC8OJZiqi"},"outputs":[],"source":["training_args = TrainingArguments(\n","\n","  # Learning rate\n","  learning_rate=1.0e-5,\n","\n","  # Number of training epochs\n","  num_train_epochs=1,\n","\n","  # Max steps to train for (each step is a batch of data)\n","  # Overrides num_train_epochs, if not -1\n","  max_steps=max_steps,\n","\n","  # Batch size for training\n","  per_device_train_batch_size=1,\n","\n","  # Directory to save model checkpoints\n","  output_dir=output_dir,\n","\n","  # Other arguments\n","  overwrite_output_dir=False, # Overwrite the content of the output directory\n","  disable_tqdm=False, # Disable progress bars\n","  eval_steps=120, # Number of update steps between two evaluations\n","  save_steps=120, # After # steps model is saved\n","  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n","  per_device_eval_batch_size=1, # Batch size for evaluation\n","  evaluation_strategy=\"steps\",\n","  logging_strategy=\"steps\",\n","  logging_steps=1,\n","  optim=\"adafactor\",\n","  gradient_accumulation_steps = 4,\n","  gradient_checkpointing=False,\n","\n","  # Parameters for early stopping\n","  load_best_model_at_end=True,\n","  save_total_limit=1,\n","  metric_for_best_model=\"eval_loss\",\n","  greater_is_better=False\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"height":273,"id":"v0BYzN9bZiqj"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPTNeoXForCausalLM(\n","  (gpt_neox): GPTNeoXModel(\n","    (embed_in): Embedding(50304, 512)\n","    (emb_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-5): 6 x GPTNeoXLayer(\n","        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n","        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n","        (attention): GPTNeoXAttention(\n","          (rotary_emb): GPTNeoXRotaryEmbedding()\n","          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n","          (dense): Linear(in_features=512, out_features=512, bias=True)\n","          (attention_dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (mlp): GPTNeoXMLP(\n","          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n","          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n","          (act): GELUActivation()\n","        )\n","      )\n","    )\n","    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",")\n","Memory footprint 0.30687256 GB\n","Flops 2195.667812352 GFLOPs\n"]}],"source":["model_flops = (\n","  base_model.floating_point_ops(\n","    {\n","       \"input_ids\": torch.zeros(\n","           (1, training_config[\"model\"][\"max_length\"])\n","      )\n","    }\n","  )\n","  * training_args.gradient_accumulation_steps\n",")\n","\n","print(base_model)\n","print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n","print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"]},{"cell_type":"code","execution_count":16,"metadata":{"height":150,"id":"Tc5Xo8Z6Ziqj"},"outputs":[],"source":["trainer = Trainer(\n","    model=base_model,\n","    model_flops=model_flops,\n","    total_steps=max_steps,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"]},{"cell_type":"markdown","metadata":{"id":"xE6D-yrAZiqj"},"source":["### Train a few steps"]},{"cell_type":"code","execution_count":17,"metadata":{"height":31,"id":"L0pmrPwlZiqj"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7c70d61505d4affa35636c0f4aa63d3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-12-08 11:30:55,263 - DEBUG - utilities - Step (1) Logs: {'loss': 3.3405, 'learning_rate': 1e-05, 'epoch': 0.0, 'iter_time': 0.0, 'flops': 0.0, 'remaining_time': 0.0}\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 3.3405, 'learning_rate': 1e-05, 'epoch': 0.0, 'iter_time': 0.0, 'flops': 0.0, 'remaining_time': 0.0}\n"]},{"name":"stderr","output_type":"stream","text":["2023-12-08 11:30:57,096 - DEBUG - utilities - Step (2) Logs: {'loss': 3.2429, 'learning_rate': 5e-06, 'epoch': 0.01, 'iter_time': 1.8331868648529053, 'flops': 1197732677693.0513, 'remaining_time': 1.8331868648529053}\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 3.2429, 'learning_rate': 5e-06, 'epoch': 0.01, 'iter_time': 1.8331868648529053, 'flops': 1197732677693.0513, 'remaining_time': 1.8331868648529053}\n"]},{"name":"stderr","output_type":"stream","text":["2023-12-08 11:30:58,962 - DEBUG - utilities - Step (3) Logs: {'loss': 3.4016, 'learning_rate': 0.0, 'epoch': 0.01, 'iter_time': 1.849397897720337, 'flops': 1187233864090.8445, 'remaining_time': 0.0}\n","2023-12-08 11:30:58,963 - DEBUG - utilities - Step (3) Logs: {'train_runtime': 5.6856, 'train_samples_per_second': 2.111, 'train_steps_per_second': 0.528, 'total_flos': 262933364736.0, 'train_loss': 3.3283351262410483, 'epoch': 0.01, 'iter_time': 1.8500678539276123, 'flops': 1186803936780.3154, 'remaining_time': 0.0}\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 3.4016, 'learning_rate': 0.0, 'epoch': 0.01, 'iter_time': 1.849397897720337, 'flops': 1187233864090.8445, 'remaining_time': 0.0}\n","{'train_runtime': 5.6856, 'train_samples_per_second': 2.111, 'train_steps_per_second': 0.528, 'train_loss': 3.3283351262410483, 'epoch': 0.01, 'iter_time': 1.8500678539276123, 'flops': 1186803936780.3154, 'remaining_time': 0.0}\n"]}],"source":["training_output = trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"kz3AA_FzZiqk"},"source":["### Save model locally"]},{"cell_type":"code","execution_count":18,"metadata":{"height":82,"id":"9Dj49RXnZiqk"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saved model to: lamini_docs_3_steps/final\n"]}],"source":["save_dir = f'{output_dir}/final'\n","\n","trainer.save_model(save_dir)\n","print(\"Saved model to:\", save_dir)"]},{"cell_type":"code","execution_count":19,"metadata":{"height":69,"id":"kmahp80iZiqk"},"outputs":[],"source":["finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"height":48,"id":"0a6wFaycZiqk"},"outputs":[{"data":{"text/plain":["GPTNeoXForCausalLM(\n","  (gpt_neox): GPTNeoXModel(\n","    (embed_in): Embedding(50304, 512)\n","    (emb_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-5): 6 x GPTNeoXLayer(\n","        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n","        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n","        (attention): GPTNeoXAttention(\n","          (rotary_emb): GPTNeoXRotaryEmbedding()\n","          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n","          (dense): Linear(in_features=512, out_features=512, bias=True)\n","          (attention_dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (mlp): GPTNeoXMLP(\n","          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n","          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n","          (act): GELUActivation()\n","        )\n","      )\n","    )\n","    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",")"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["finetuned_slightly_model.to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"WJwzqp6uZiqk"},"source":["### Run slightly trained model"]},{"cell_type":"code","execution_count":21,"metadata":{"height":99,"id":"TdWBdWr7Ziqk"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n","Finetuned slightly model's answer: \n","\n","\n","I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamin\n"]}],"source":["test_question = test_dataset[0]['question']\n","print(\"Question input (test):\", test_question)\n","\n","print(\"Finetuned slightly model's answer: \")\n","print(inference(test_question, finetuned_slightly_model, tokenizer))"]},{"cell_type":"code","execution_count":22,"metadata":{"height":48,"id":"8Dt3Oy8wZiql"},"outputs":[{"name":"stdout","output_type":"stream","text":["Target answer output (test): Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n"]}],"source":["test_answer = test_dataset[0]['answer']\n","print(\"Target answer output (test):\", test_answer)"]},{"cell_type":"markdown","metadata":{"id":"y3lUJVJxZiql"},"source":["### Run same model trained for two epochs"]},{"cell_type":"code","execution_count":23,"metadata":{"height":137,"id":"PxWEBLW5Ziql"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b5e8827fac348ff8df02bd8a5922643","version_major":2,"version_minor":0},"text/plain":["Downloading config.json:   0%|          | 0.00/717 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a354dd3bf31f4538b30238e08c4867bb","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/282M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6649102fd824731900748f5d7c36955","version_major":2,"version_minor":0},"text/plain":["Downloading generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f71406b47b434b42a41f060f9dc0565c","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c85994d021f64d7c8a997caf379b0bbb","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31998d368cd34861abfcefd0342fbc9e","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Finetuned longer model's answer: \n","Yes, Lamini can generate technical documentation or user manuals for software projects. This can be achieved by providing a prompt for a specific technical question or question to the LLM Engine, or by providing a prompt for a specific technical question or question. Additionally, Lamini can be trained on specific technical questions or questions to help users understand the process and provide feedback to the LLM Engine. Additionally, Lamini\n"]}],"source":["finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n","tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n","\n","finetuned_longer_model.to(device)\n","print(\"Finetuned longer model's answer: \")\n","print(inference(test_question, finetuned_longer_model, tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"9a3bXm5BZiqn"},"source":["### Explore moderation using small model\n","First, try the non-finetuned base model:"]},{"cell_type":"code","execution_count":24,"metadata":{"height":86,"id":"tGg89-gJZiqn"},"outputs":[],"source":["base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n","base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","I think I’m going to go to the next page.\n","\n","I think I’m going to go to the next page.\n","\n","I think I’m going to go to the next page.\n","\n","I think I’m going to go to the next page.\n","\n","I think I’m going to go to the next page.\n","\n","I think I’m going to go to the next page.\n","\n","I\n"]}],"source":["print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
